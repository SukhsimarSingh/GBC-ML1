{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIFA 19 Player Skills : https://github.com/SukhsimarSingh/GBC-ML1/blob/main/fifa19.csv\n",
    "\n",
    "Raw file link : https://raw.githubusercontent.com/SukhsimarSingh/GBC-ML1/main/fifa19.csv\n",
    "\n",
    "Attributes: FIFA 2019 players attributes like Age, Nationality, Overall, Potential, Club, Value, Wage, Preferred Foot, International Reputation, Weak Foot, Skill Moves, Work Rate, Position, Jersey Number, Joined, Loaned From, Contract Valid Until, Height, Weight, LS, ST, RS, LW, LF, CF, RF, RW, LAM, CAM, RAM, LM, LCM, CM, RCM, RM, LWB, LDM, CDM, RDM, RWB, LB, LCB, CB, RCB, RB, Crossing, Finishing, Heading, Accuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression, Interceptions, Positioning, Vision, Penalties, Composure, Marking, StandingTackle, SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, and Release Clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now the Feature selector works only for fifa19.csv dataset, but the aim is to make it generalized for any dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from collections import Counter\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Functions for Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y,num_feats):\n",
    "\n",
    "    cor_list = []\n",
    "    global feature_name \n",
    "    feature_name = X.columns.tolist()\n",
    "    # calculate the correlation with for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor  = np.corrcoef(X[i], y)[0,1]\n",
    "        cor_list.append(cor)\n",
    "\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "\n",
    "    return cor_support, cor_feature\n",
    "\n",
    "def chi_squared_selector(X, y, num_feats):\n",
    "\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    chi_selector = SelectKBest(chi2, k = num_feats)\n",
    "    chi_selector.fit(X_norm, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:, chi_support].columns.tolist()\n",
    "\n",
    "    return chi_support, chi_feature\n",
    "\n",
    "def rfe_selector(X, y, num_feats):\n",
    "\n",
    "    rfe_selector = RFE(estimator =LogisticRegression(),\n",
    "                      n_features_to_select = num_feats,\n",
    "                      step =10, verbose =5\n",
    "                      )\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    rfe_selector.fit(X_norm,y)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = X.loc[:, rfe_support].columns.tolist()\n",
    "\n",
    "    return rfe_support, rfe_feature\n",
    "\n",
    "def embedded_log_reg_selector(X, y, num_feats):\n",
    "\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    embedded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), max_features=num_feats)\n",
    "    embedded_lr_selector.fit(X_norm,y)\n",
    "    \n",
    "    embedded_lr_support = embedded_lr_selector.get_support()\n",
    "    embedded_lr_feature = X.loc[:,embedded_lr_support].columns.tolist()\n",
    "\n",
    "    return embedded_lr_support, embedded_lr_feature\n",
    "\n",
    "def embedded_rf_selector(X, y, num_feats):\n",
    "\n",
    "    embedded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\n",
    "    embedded_rf_selector.fit(X,y)\n",
    "    \n",
    "    embedded_rf_support = embedded_rf_selector.get_support()\n",
    "    embedded_rf_feature = X.loc[:, embedded_rf_support].columns.tolist()\n",
    "\n",
    "    return embedded_rf_support, embedded_rf_feature\n",
    "\n",
    "def embedded_lgbm_selector(X, y, num_feats):\n",
    "\n",
    "    lgbc = LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves =32, colsample_bytree=0.2,\n",
    "                         reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    embedded_lgbm_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
    "    embedded_lgbm_selector.fit(X,y)\n",
    "    \n",
    "    embedded_lgbm_support = embedded_lgbm_selector.get_support()\n",
    "    embedded_lgbm_feature = X.loc[:, embedded_lgbm_support].columns.tolist()\n",
    "\n",
    "    return embedded_lgbm_support, embedded_lgbm_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path):\n",
    "    # Your code starts here (Multiple lines)\n",
    "    player_df = pd.read_csv(dataset_path)\n",
    "    numcols = ['Overall', 'Crossing','Finishing',  'ShortPassing',  'Dribbling','LongPassing', 'BallControl', 'Acceleration','SprintSpeed', 'Agility',  'Stamina','Volleys','FKAccuracy','Reactions','Balance','ShotPower','Strength','LongShots','Aggression','Interceptions']\n",
    "    catcols = ['Preferred Foot','Position','Body Type','Nationality','Weak Foot']\n",
    "    player_df = player_df[numcols+catcols]\n",
    "    traindf = pd.concat([player_df[numcols], pd.get_dummies(player_df[catcols])],axis=1)\n",
    "    features = traindf.columns\n",
    "\n",
    "    traindf = traindf.dropna()\n",
    "    traindf = pd.DataFrame(traindf,columns=features)\n",
    "    \n",
    "    y = traindf['Overall']>=87\n",
    "    X = traindf.copy()\n",
    "    del X['Overall']\n",
    "    \n",
    "    feature_name = list(X.columns)\n",
    "    \n",
    "    num_feats = 30\n",
    "\n",
    "    return X, y, num_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path of Dataset:  https://raw.githubusercontent.com/SukhsimarSingh/GBC-ML1/main/fifa19.csv\n",
      "Enter the methods used with 1 space between them:  pearson rfe\n",
      "Fitting estimator with 223 features.\n",
      "Fitting estimator with 213 features.\n",
      "Fitting estimator with 203 features.\n",
      "Fitting estimator with 193 features.\n",
      "Fitting estimator with 183 features.\n",
      "Fitting estimator with 173 features.\n",
      "Fitting estimator with 163 features.\n",
      "Fitting estimator with 153 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 33 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:85: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best features are as follows:  ['Volleys', 'ShortPassing', 'Reactions', 'LongPassing', 'Finishing']\n",
      "Press Enter to Close the Program :  \n"
     ]
    }
   ],
   "source": [
    "def autoFeatureSelector(dataset_path, methods=[]):\n",
    "    # Parameters\n",
    "    # data - dataset to be analyzed (csv file)\n",
    "    # methods - various feature selection methods we outlined before, use them all here (list)\n",
    "    \n",
    "    # preprocessing\n",
    "    X, y, num_feats = preprocess_dataset(dataset_path)\n",
    "    \n",
    "    # Run every method we outlined above from the methods list and collect returned best features from every method\n",
    "    if 'pearson' in methods:\n",
    "        cor_support, cor_feature = cor_selector(X, y,num_feats)\n",
    "    if 'chi-square' in methods:\n",
    "        chi_support, chi_feature = chi_squared_selector(X, y,num_feats)\n",
    "    if 'rfe' in methods:\n",
    "        rfe_support, rfe_feature = rfe_selector(X, y,num_feats)\n",
    "    if 'log-reg' in methods:\n",
    "        embedded_lr_support, embedded_lr_feature = embedded_log_reg_selector(X, y, num_feats)\n",
    "    if 'rf' in methods:\n",
    "        embedded_rf_support, embedded_rf_feature = embedded_rf_selector(X, y, num_feats)\n",
    "    if 'lgbm' in methods:\n",
    "        embedded_lgbm_support, embedded_lgbm_feature = embedded_lgbm_selector(X, y, num_feats)\n",
    "    \n",
    "    \n",
    "    # Combining all the above feature list and count the maximum set of features that got selected by all methods\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    # put all selection together\n",
    "    feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embedded_lr_support,\n",
    "                                    'Random Forest':embedded_rf_support, 'LightGBM':embedded_lgbm_support})\n",
    "    # Counting the selected times for each feature\n",
    "    feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "    # Displaying the top 100\n",
    "    feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "    feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "    feature_selection_df.head(num_feats)\n",
    "    \n",
    "    best_features = list(feature_selection_df[\"Feature\"].head())\n",
    "\n",
    "    return best_features\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_path = input(\"Enter the path of Dataset:  \")\n",
    "    methods = input(\"Enter the methods used with 1 space between them:  \").lower().split()\n",
    "    best_features = autoFeatureSelector(dataset_path, methods=['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm'])\n",
    "    print(\"The best features are as follows: \",best_features)\n",
    "    close = input(\"Press Enter to Close the Program :  \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
